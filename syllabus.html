<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Syllabus | Principles of Machine Learning</title>

  <!-- bootstrap -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap-theme.min.css">

  <!-- Google fonts -->
  <link href='https://fonts.googleapis.com/css?family=Roboto:700,400,300' rel='stylesheet' type='text/css'>
  <!-- <link href='http://fonts.googleapis.com/css?family=Lato:400,300' rel='stylesheet' type='text/css'> -->

  <!-- Google Analytics -->
  <script>
  </script>

  <link rel="stylesheet" type="text/css" href="style.css" />
</head>

<body>

<div id="header">

  <a href="index.html">
    <h1>IDS 705: Principles of Machine Learning</h1>
  </a>
  <div class='text-center'>
    <h4>Duke University</h4>
    <h4>Spring 2020</h4>
    <h3>Previous Years: <a href="https://kylebradbury.github.io/ids705_sp2019/syllabus.html">2019</a>, <a href="https://kylebradbury.github.io/ece590/syllabus.html">2018</a></h3>
  </div>

  <div style="clear:both;"></div>
</div>


<div class="container sec">

  <h2>Schedule and Syllabus</h2>

  <br>
  The schedule below is a guide to what we will be covering throughout the semester and is subject to change to meet the learning goals of the class. Check this website regularly for the latest schedule and for course materials that will be posted here through links on the syllabus.<br>
  <i>ISL = <a href='http://faculty.marshall.usc.edu/gareth-james/ISL/'>Introduction to Statistical Learning</a>, by James, Witten, Hastie, and Tibshirani</i><br>
  <i>DM = <a href='https://www-users.cs.umn.edu/~kumar001/dmbook/'>Introduction to Data Mining</a>, by Tan, Steinbach, Karpatne, and Kumar</i><br>
  <i>PRML = <a href="https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book">Pattern Recognition and Machine Learning</a>, by Bishop</i><br>
  <i>DL = <a href='https://www.deeplearningbook.org/'>Deep Learning</a>, by Goodfellow, Bengio, and Courville</i><br>
  <i>RL = <a href='http://incompleteideas.net/book/the-book.html'>Reinforcement Learning: An Introduction</a>, by Sutton and Barto</i><br>

</div>


<!-- <tr class="warning">    <tr class="danger">    <tr class="info"> -->

<div class="container sec">
<table class="table table-hover">
  <thead class="thead-light">
    <tr>
      <th>Event Type</th><th>Date</th><th>Description</th><th>Readings</th><th>Course Materials</th>
    </tr>
  </thead>
  <tr>
    <td class="lecnum">Lecture 1</td>
    <td class="date">Wednesday<br> Jan 8</td>
    <td>
      <b>What is machine learning?</b> <br>
      Course overview and an orientation to the major branches of machine learning: unsupervised, supervised, and reinforcement learning 
    </td>
    <td>None</td>
    <td>
      <a href="https://github.com/kylebradbury/ids705/raw/master/lectures/lecture01_what_is_machine_learning.pdf">[slides]</a>
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 2</td>
    <td class="date">Monday <br> Jan 13</td>
    <td>
      <b>An end-to-end machine learning example</b> <br>
      Stating the problem, creating the model, evaluating performance, and operationalizing the solution. </td>
    <td>ISL Ch. 1 + 2.1</td>
    <td>
      <a href="https://github.com/kylebradbury/ids705/raw/master/lectures/lecture02_end-to-end_machine_learning.pdf">[slides]</a><br>
      <a href="https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb">[sample code]</a>
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 3</td>
    <td class="date">Wednesday <br> Jan 16</td>
    <td>
      <b>How flexible should my algorithms be: the bias-variance tradeoff </b> <br>
      K-nearest neighbors classification and the bias-variance tradeoff 
    </td>
    <td>ISL 2.2</td>
    <td>
      <a href="https://github.com/kylebradbury/ids705/raw/master/lectures/lecture03_bias_variance_tradeoff.pdf">[slides]</a>
    </td>
  </tr>

  <tr class="table-primary">
    <td class="lecnum">No class</td>
    <td class="date">Monday<br> Jan 20</td>
    <td>Martin Luther King, Jr. Day</td>
    <td></td>
    <td></td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date">Wednesday<br> Jan 22</td>
    <td><b>Assignment #1 Due</b></td>
    <td></td>
    <td><a href="https://github.com/kylebradbury/ids705/blob/master/assignments/Assignment_1.ipynb">[assignment]</a></td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 4</td>
    <td class="date">Wednesday <br> Jan 22</td>
    <td>
      <b>Linear Models I</b> <br>
      Simple linear regression, multiple linear regression, measuring error, model fitting and least squares, comparing linear regression and classification
    </td>
    <td>ISL Intro of 3, 3.1, and 3.2</td>
    <td>
      <a href="https://github.com/kylebradbury/ids705/raw/master/lectures/lecture04_linear_models.pdf">[slides]</a>
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 5</td>
    <td class="date">Monday <br> Jan 27</td>
    <td>
      <b>Linear Models II</b> <br>
      Nonlinear transformations of predictors, cost/loss functions, selecting parameters through gradient descent.
    </td>
    <td>ISL 3.3 and 3.5</td>
    <td>
      [slides]
    </td>
  </tr>
  
  <tr>
    <td class="lecnum">Lecture 6</td>
    <td class="date">Wednesday <br> Jan 29</td>
    <td>
      <b>Performance evaluation and model comparison</b> <br>
      Choosing the right mode: accuracy vs speed vs interpretability; metrics for supervised learning performance evaluation: types of errors, receiver operating characteristics curves, confusion matrices
    </td>
    <td>ISL 4.1, 4.2, and 4.3</td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 7</td>
    <td class="date">Monday <br> Feb 3</td>
    <td>
      <b>Validation and model testing</b> <br>
      Resampling techniques: training, testing, and validation datasets, the importance of ensuring representative resampling, and cross validation
    </td>
    <td>ISL 5.1 and 5.2</td>
    <td>
      [slides]
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date">Wednesday<br> Feb 5</td>
    <td><b>Assignment #2 Due</b></td>
    <td></td>
    <td><a href="https://github.com/kylebradbury/ids705/blob/master/assignments/Assignment_2.ipynb">[assignment]</a></td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 8</td>
    <td class="date">Wednesday <br> Feb 5</td>
    <td>
      <b>Decision theory</b> <br>
      How to operate supervised learning algorithms in practice
    </td>
    <td><a href="http://canmedia.mheducation.ca/college/olcsupport/lind/5ce/Lind5ce_Ch17_An_Introduction_to_Decision_Theory.pdf">Link to reading</a></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>  
    <td class="lecnum">Lecture 9</td>
    <td class="date">Monday <br> Feb 10</td>
    <td>
      <b>Reducing overfit</b> <br>
      Model and feature selection; Occam’s razor; Subset selection; L1 (ridge), L2 (LASSO), and elastic net regularization.
    </td>
    <td>ISL 6.1 and 6.2</td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 10</td>
    <td class="date">Wednesday <br> Feb 12 <br> Class starts 10 min late</td>
    <td>
      <b>Additional classification methods</b> <br>
      Linear discriminant analysis and naïve Bayes
    </td>
    <td>ISL 4.4 and 4.5</td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 11</td>
    <td class="date">Monday <br> Feb 17</td>
    <td>
      <b>Tree-based models and ensembles</b> <br>
      From decision trees to random forests: bagging, bootstrapping, and boosting
    </td>
    <td>ISL 8.1 and 8.2</td>
    <td>
      [slides]
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date">Wednesday<br> Feb 19</td>
    <td><b>Assignment #3 Due</b></td>
    <td></td>
    <td>[assignment]</td>
  </tr>



  <tr>
    <td class="lecnum">Lecture 12</td>
    <td class="date">Wednesday <br> Feb 19</td>
    <td>
      <b>Dimensionality reduction</b> <br>
      The Curse of Dimensionality and intro to principal components analysis (PCA)
    </td>
    <td>ISL 6.3, 6.4, 10.1, and 10.2</td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 13</td>
    <td class="date">Monday <br> Feb 24</td>
    <td>
      <b>Principal components analysis (PCA)</b> <br>
      Explaining how PCA works and how we calculate the principal components.
    </td>
    <td>ISL 10.3</td>
    <td>
      [slides]
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date">Wednesday<br> Feb 26</td>
    <td><b>End of Kaggle Competition</b></td>
    <td></td>
    <td>[kaggle competition]</td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 14</td>
    <td class="date">Wednesday <br> Feb 26</td>
    <td>
      <b>Clustering I</b> <br>
      From K-means to Gaussian mixture model clustering and Expectation Maximization
    </td>
    <td>DM Ch 7 (<a href="https://www-users.cs.umn.edu/~kumar001/dmbook/ch7_clustering.pdf">link</a>): Intro, 7.1 and 7.2</td>
    <td>
      [slides]
      <br>
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 15</td>
    <td class="date">Monday <br> Mar 2</td>
    <td>
      <b>Clustering II</b> <br>
      Hierarchical clustering, DBSCAN, and spectral clustering
    </td>
   <td>DM Ch 7 (<a href="https://www-users.cs.umn.edu/~kumar001/dmbook/ch7_clustering.pdf">link</a>): 7.3 and 7.4</td>
   <td>
     [slides]
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date">Wednesday<br> Mar 4</td>
    <td><b>Kaggle Competition Reports Due</b></td>
    <td></td>
    <td>[kaggle report]</td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 16</td>
    <td class="date">Wednesday <br> Mar 4</td>
    <td>
      <b>Neural networks I</b> <br>
      How a neural network works
    </td>
    <td>PRML Ch 5: 5.1 </td>
    <td>
      [slides]
    </td>
  </tr>

    <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date">Friday<br> Mar 6</td>
    <td><b>Kaggle Team Peer Evaluation</b></td>
    <td></td>
    <td>[peer evaluation]</td>
  </tr>

  <tr class="table-primary">
    <td class="lecnum">No class</td>
    <td>Mar 9-13</td>
    <td>Spring break week</td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 17</td>
    <td class="date">Monday <br> Mar 16</td>
    <td>
      <b>Neural networks II</b> <br>
      Backpropagation
    </td>
    <td>PRML Ch 5: 5.3 (intro), 5.3.1, 5.3.2, and <a href="http://colah.github.io/posts/2015-08-Backprop/">Calculus on Computational Graphs</a></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 18</td>
    <td class="date">Wednesday <br> Mar 18</td>
    <td>
      <b>Introduction to Deep learning</b> <br>
      Implementing deep learning models in Keras
    </td>
    <td>DL Ch 11: Practical Methodology</td>
    <td>
      [slides]
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date">Monday <br> Mar 23</td>
    <td><b>Assignment #4 Due</b></td>
    <td></td>
    <td>[assignment]</td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 19</td>
    <td class="date">Monday <br> Mar 23</td>
    <td>
      <b>Reinforcement Learning I</b> <br>
      Formulating the reinforcement learning problem
    </td>
    <td>RL Ch 1: 1.1-1.6; Ch 2: 2.1-2.5</td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 20</td>
    <td class="date">Wednesday <br> Mar 25</td>
    <td>
      <b>Reinforcement Learning II</b> <br>
      Policy and value functions, rewards, and introduction to Markov processes 
    </td>
    <td>RL Ch 3</td>
    <td>
      [slides]
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date">Monday <br> Mar 30</td>
    <td><b>Final Project Proposal</b></td>
    <td></td>
    <td>[Final Project]</td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 21</td>
    <td class="date">Monday <br> March 30</td>
    <td>
      <b>Reinforcement Learning III</b> <br>
      From Markov Chains to Markov Decision Processes (MDPs)
    </td>
    <td>RL Ch 4</td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 22</td>
    <td class="date">Wednesday <br> Apr 1</td>
    <td>
      <b>Reinforcement Learning IV</b> <br>
      Finding optimal policies through policy iteration, value iteration, and Monte Carlo methods
    </td>
    <td>RL Ch 5: 5.1-5.3</td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 23</td>
    <td class="date">Monday <br> Apr 6</td>
    <td>
      <b>Kernel Smoothing</b> <br>
      Non-parametric methods including kernel density estimation, kernel regression, and local regression
    </td>
    <td>None</td>
    <td>
      [slides]
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date">Monday <br> Apr 6</td>
    <td><b>Assignment #5 Due</b></td>
    <td></td>
    <td>
     [assignment]
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 24</td>
    <td class="date">Wednesday <br> Apr 8</td>
    <td>
      <b>Kernel Methods</b> <br>
      Introducing Kernel machines via the kernel perceptron, maximum margin classifiers, and support vector machines
    </td>
    <td>ISL Ch 9: 9.1-9.4</td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 25</td>
    <td class="date">Monday <br> Apr 13</td>
    <td>
      <b>State-of-the-art machine learning and applications</b> <br>
      Cutting-edge applications and techniques: ideas on where the field is heading and how to stay up-to-date
    </td>
    <td>None</td>
    <td>
      [slides]
    </td>
  </tr>

  <tr class="table-danger">
    <td><b>Deliverable</b></td>
    <td class="date">Wednesday <br> Apr 15</td>
    <td>
      <b>Final project showcase and competition (last class meeting of the semester)</b>
    </td>
    <td></td>
    <td>
      [Final Project]
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date">Thursday<br> Apr 16</td>
    <td><b>Final Report</b></td>
    <td></td>
    <td>[Final Project]</td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date">Friday<br> Apr 17</td>
    <td><b>Final Project Peer Evaluation</b></td>
    <td></td>
    <td>[peer evaluation]</td>
  </tr>

</table>
</div>

<div class="sechighlight">
<div id="footer">
  <div id="footermsg">Website design adapted from the <a href="http://cs231n.stanford.edu/">Stanford CS231 course page</a></div>
</div>
</div>

<!-- jQuery and Boostrap -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
</body>

</html>
