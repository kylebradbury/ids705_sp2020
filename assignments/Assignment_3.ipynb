{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *YOUR FULL NAME HERE*\n",
    "Netid: Your netid here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions for all assignments can be found [here](https://github.com/kylebradbury/ids705/blob/master/assignments/_Assignment%20Instructions.ipynb), which is also linked to from the [course syllabus](https://kylebradbury.github.io/ids705/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives:\n",
    "This assignment will provide structured practice to help enable you to...\n",
    "1. Understand the primary workflow in machine learning: (1) identifying a hypothesis function set of models, (2) determining a loss/cost/error/objective function to minimize, and (3) minimizing that function through gradient descent\n",
    "2. Implement batch gradient descent and become familiar with how that technique is used and its dependence on the choice of learning rate\n",
    "3. Gain practice in implementing machine learning algorithms to understand the math and programming behind them to achieve practical proficiency with the techniques\n",
    "4. Evaluate supervised learning algorithm performance through ROC curves and using cross validation\n",
    "5. How to work with imagery data and the basics of computer vision approaches to machine learning\n",
    "6. Develop and understanding of Bayes rule and the optimal minimum misclassification error classifier (Bayes' Rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MAC USERS TAKE NOTE:\n",
    "# For clearer plots in Jupyter notebooks on macs, run the following line of code:\n",
    "# %config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "### [40 points] From theory to practice: classification through logistic regression\n",
    "\n",
    "#### Introduction\n",
    "For this problem you will derive, implement through gradient descent, and test the performance of a logistic regression classifier for a binary classification problem.\n",
    "\n",
    "In this case, we'll assume our logistic regression problem will be applied to a two dimensional feature space. Our logistic regression model is:\n",
    "\n",
    "$$f(\\mathbf{x}_i,\\mathbf{w})=\\sigma(\\mathbf{w}^{\\intercal} \\mathbf{x}_i)$$\n",
    "\n",
    "where the sigmoid function is defined as $\\sigma(x) = \\dfrac{e^x}{1+e^{x}}= \\dfrac{1}{1+e^{-x}}$. Also, since this is a two-dimensional problem, we define $\\mathbf{w}^{\\intercal} \\mathbf{x}_i = w_0 x_{i,0} + w_1 x_{i,1} + w_2 x_{i,2}$ and here, $\\mathbf{x}_i=[x_{i,0}, x_{i,1}, x_{i,2}]^{\\intercal}$, and $x_{i,0} \\triangleq 1$\n",
    "\n",
    "As in class, we will interpret the response of the logistic regression classifier to be the likelihood of the data given the model parameters. For one sample, $(y_i, \\mathbf{x}_i)$, this is given as:\n",
    "\n",
    "$$P(Y=y_i|X=\\mathbf{x}_i) = f(\\mathbf{x}_i,\\mathbf{w})=\\sigma(\\mathbf{w}^{\\intercal} \\mathbf{x}_i)$$\n",
    "\n",
    "*Aside: the careful reader will recognize this expression looks different from when we talk about the likelihood of our data given the true class label, typically expressed as $P(x|y)$, or the posterior probability of a class label given our data, typically expressed as $P(y|x)$. In the context of training a logistic regression model, we know the training $\\mathbf{x}$ values and $y$ values, so the above probability is primarily a function of the logistic regression parameters, $\\mathbf{w}$. It's our goal to use this to choose the parameters to maximize the probability of our data by adjusting our model*\n",
    "\n",
    "#### Find the cost function that we can use to choose the model parameters, $\\mathbf{w}$, that best fit the training data.\n",
    "\n",
    "**(a)** What is the likelihood function for all the $N$ samples in our training dataset that we will wish to maximize?\n",
    "\n",
    "**(b)** Since a logarithm is a monotonic function, maximizing the $f(x)$ is equivalent to maximizing $\\ln [f(x)]$. Express part (a) as a cost function of the model parameters, $C(\\mathbf{w})$, that is the negative of the logarithm of (a).\n",
    "\n",
    "**(c)** Calculate the gradient of the cost function with respect to the model parameters $\\nabla_{\\mathbf{w}}C(\\mathbf{w})$. Express this in terms of the partial derivatives of the cost function with respect to each of the parameters, e.g. $\\nabla_{\\mathbf{w}}C(\\mathbf{w}) = \\left[\\dfrac{\\partial C}{\\partial w_0}, \\dfrac{\\partial C}{\\partial w_1}, \\dfrac{\\partial C}{\\partial w_2}\\right]$.\n",
    "\n",
    "**(d)** Write out the gradient descent update equation, assuming $\\eta$ represents the learning rate.\n",
    "\n",
    "#### Prepare and plot your data\n",
    "\n",
    "**(e)** Load the data and scatter plot the data by class. In the data folder in the same directory of this notebook, you'll find the data in `A3_Q1_data.csv`. This file contains the binary class labels, $y$, and the features $x_1$ and $x_2$.  Comment on the data: do the data appear separable? Why might logistic regression be a good choice for these data or not?\n",
    "\n",
    "**(f)** Do the data require any preprocessing due to missing values, scale differences, etc? If so, how did you remediate this?\n",
    "\n",
    "#### Implement gradient descent and your logistic regression algorithm\n",
    "\n",
    "**(g)** Create a function or class to implement your logistic regression. It should take as inputs the model parameters, $\\mathbf{w}=\\left[w_0,w_1,w_2\\right]^{\\intercal}$, and output the class confidence probabilities, $P(Y=y_i|X=\\mathbf{x}_i)$.\n",
    "\n",
    "**(h)** Create a function that computes the cost function $C(\\mathbf{w})$ for a given dataset and corresponding class labels.\n",
    "\n",
    "**(i)** Create a function or class to run gradient descent on your training data. We'll refer to this as \"batch\" gradient descent since it takes into account the gradient based on all our data at each iteration (or \"epoch\") of the algorithm.  In doing this we'll need to make some assumptions about and/or experiment with the following:\n",
    "1. The initialization of the algorithm - what should you initialize the model parameters to? For this, randomly initialize the weights to a different values between 0 and 1.\n",
    "2. The learning rate - how slow/fast should the algorithm proceed in the direction opposite the gradient? This you will experiment with.\n",
    "3. Stopping criteria - when should the algorithm be finished searching for the optimum? Set this to be when the cost function changes by no more than $10^{-6}$ between iterations. Since we have a weight vector, you can compute this by seeing if the L2 norm of the weight vector changes by no more than $10^{-6}$ between iterations.\n",
    "\n",
    "Please compute your cost function for a batch as the average cost for the data in your current batch (in this case, a batch is your entire training dataset). In other words, divide your cost by the number of samples in each batch.\n",
    "\n",
    "**(j)** Design your approach so that at each step in the gradient descent algorithm it will produce updated parameter estimates. For each set of estimates, calculate the cost function for both the training and the test data (no output is necessary here, but this is used in the following question).\n",
    "\n",
    "**(k)** Divide your data into a training and testing set where the test set accounts for 30 percent of the data and the training set the remaining 70 percent. Show the gradient descent process for different learning rates by plotting the resulting cost as a function of each iteration (or \"epoch\"). What is the impact that each parameter has on the process and the results? What choices did you make in your chosen approach and why? Use the parameter you choose here for the learning rate for the remainder of this question. \n",
    "\n",
    "#### Test your model performance through cross validation\n",
    "\n",
    "**(l)** Test the performance of your trained classifier using K-folds cross validation (while this can be done manually, the scikit-learn package [StratifiedKFolds](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold) may be helpful). Produce Receiver Operating Characteristic curves (ROC curves) of your cross validated performance. \n",
    "\n",
    "**(m)** Why do we use cross validation?\n",
    "\n",
    "**(n)** Make two plots - one of your training data, and one for your test data - with the data scatter-plot and the decision boundary for your classifier. Comment on your decision boundary. Could it be improved?\n",
    "\n",
    "**(o)** Compare your trained model to random guessing. Show the ROC curve for your model and plot the chance diagonal. What area under the curve (AUC) does your model achieve? How does your model compare in terms of performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "### [20 points] Digits classification\n",
    "\n",
    "**(a)** Construct your dataset from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) of handwritten digits, which has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "Your goal is to determine whether or not an example is a 3, therefore your binary classifier will seek to estimate $y=1$ if the digit is a 3, and $y=0$ otherwise. Create your dataset by transforming your labels into a binary format. \n",
    "\n",
    "**(b)** Plot 10 examples of each class (i.e. class $y=0$, which are not 3's and class $y=1$ which are 3's), from the training dataset.\n",
    "\n",
    "**(c)** How many examples are present in each class? Show a histogram of samples by class. Are the classes balanced? What issues might this cause?\n",
    "\n",
    "**(d)** Using cross-validation, train and test a classifier. Compare your performance against (1) a classifier that randomly guesses the class, and (2) a classifier that guesses that all examples are NOT 3's. Plot corresponding ROC curves and precision-recall curves. Describe the algorithm's performance and explain any discrepancies you find.\n",
    "\n",
    "**(f)** Using a logistic regression classifier (a linear classifier), apply lasso regularization and retrain the model and evaluate its performance over a range of values on the regularization coefficient. You can implement this using the [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) module (DO NOT use your function from question 1) and activating the 'l1' penalty; the parameter $C$ is the inverse of the regularization strength. As you vary the regularization coefficient, plot (1) the number of model parameters that are estimated to be nonzero; (2) the logistic regression cost function, which you created a function to evaluate in the Question 1; (3) $F_1$-score, and (4) area under the curve (AUC). Describe the implications of your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "### [10 points] Comparing the Bayes' decision rule with logistic regression\n",
    "\n",
    "The phrase \"Bayes' decision rule\" is often used to describe a classifier decision rule that minimizes misclassification rate (equally penalizing false positives and false negatives). In this exercise you will first determine the Bayes's decision rule for a binary classification problem where you know the likelihood of date from each class. The binary classification problem has two classes with data distributed as exponential random variables:\n",
    "\n",
    "$$P(x|C_i) = \\lambda_i e^{-\\lambda_i x}$$\n",
    "\n",
    "Where $C_i$ represents the class from which the sample is drawn (0 or 1). We also know that $\\lambda_0 = 1$ and $\\lambda_1 = 5$.\n",
    "\n",
    "**(a)** Plot the probability of each class conditional distribution (e.g. likelihood function), $P(x|C_0)$ and $P(x|C_1)$ on the sample plot in the domain $x \\in [0,2]$. *You can use [`scipy`'s `expon` module](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.expon.html#scipy.stats.expon) for this. Note that the `scale` parameter for this module is defined as $1/\\lambda$.*\n",
    "\n",
    "**(b)** Assuming the prior class distributions are $P(C_0)=P(C_1)=0.5$, determine the Bayes' decision rule using the information above including the posterior distributions and the prior distributions. You will assign the most probable class to the data based on the value of $x$. The decision rule will be of the form: \n",
    "\n",
    "If $x > x^*$, then predict Class 0, otherwise predict Class 1\n",
    "\n",
    "Determine the value $x^*$ that minimizes misclassification (equally penalizing false positives and false negatives, and no penalty/reward for correct detections). Show your work in deriving this value.\n",
    "\n",
    "**(c)** How does your answer in (b) relate to the the plot you made in (a)? Why?\n",
    "\n",
    "**(d)** Load the test data in the file `A3_Q3_test.csv`. Apply your decision rule to the data. What is the misclassification rate (error rate, or fraction of misclassified samples) of this decision rule?\n",
    "\n",
    "**(e)** Load the training data in the file `A3_Q3_train.csv` and train a logistic regression classifier on the data (using default parameters). What is your misclassification error for your test dataset? How does this compare with the Bayes' classifier?\n",
    "\n",
    "**(f)** What is your decision rule for your logistic regression model? To compute this, extract the parameters from your fit model (look for the `coef_` and `intercept_` attributes) and since the classes are balanced, the decision rule will be to classify a sample $x$ as Class 1 when your logistic regression sigmoid is greater than 0.5 (the halfway point from the two extremes of 0 and 1). How does this compare with the Bayes' classifier?\n",
    "\n",
    "**(g)** If the prior probabilities were not $P(C_0)=P(C_1)=0.5$, but instead if P(C_1)>P(C_0). How would this impact the optimal decision rule? Would it change, if so, would $x^*$ be larger or smaller?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "\n",
    "### [30 points] Supervised learning exploration\n",
    "\n",
    "For this exercise, you will construct and implement a supervised learning problem solution/experiment. Describe your process and answer these questions clearly and thoroughly. Part of the grade in this assignment is devoted to the quality and professionalism of your work.\n",
    "\n",
    "**(a)** Identify a question or problem that's of interest to you and that could be addressed using classification or regression. Explain why it's interesting and what you'd like to accomplish. You're encouraged to be creative.\n",
    "\n",
    "**(b)** Download the data and plot the data to describe it. You can use any dataset of interest to you with the exception of the Iris dataset, the Kaggle Titanic dataset, or the Kaggle chocolate dataset. Possible sources of dataset include (but are not limited to):\n",
    "\n",
    "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)\n",
    "- [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
    "- [Amazon Open Datasets](https://registry.opendata.aws/)\n",
    "- [Microsoft's Open Data](https://msropendata.com/)\n",
    "- [Google's Dataset Search](https://datasetsearch.research.google.com/)\n",
    "- [Awesomedata's list of datasets](https://github.com/awesomedata/awesome-public-datasets)\n",
    "\n",
    "**(c)** Formulate your supervised learning question: (a) What is your target variable (what are you trying to predict) and what predictors do you have available? Does your dataset require any preprocessing: is it clean (no missing values or erroneous data) and normalized (are each of the predictors of the same magnitude)? \n",
    "\n",
    "**(d)** What supervised learning technique will you use and why? \n",
    "\n",
    "**(e)** How will you evaluate performance and know whether you succeeded (e.g. ROC curves for binary classification, mean square error or $R^2$ for regression)?\n",
    "\n",
    "**(f)** Divide your dataset into training and testing datasets OR implement cross validation. Explain your approach and why you adopted it.\n",
    "\n",
    "**(g)** Run your analysis and show your performance. Include plots of your data and of performance.\n",
    "\n",
    "**(h)** Describe how your system performed, where your supervised learning algorithm performed well, where it did not, and how you could improve it. Summarize the conclusions from your work (this should involve a degree of interpretation more so than \"my classifier achieved an AUC of 0.8\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This grade for this question is based on (a) the completeness of the answer in addressing each of the above components, (b) the quality of the work and accuracy of the interpretation of the results of the analysis, (c) the presentation and professionalism of the work, and (d) the amount of creativity demonstrated by the work.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "722px",
    "left": "1550px",
    "right": "20px",
    "top": "121px",
    "width": "353px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
